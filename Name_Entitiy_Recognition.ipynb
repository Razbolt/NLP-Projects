{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWvnWFHt_TqY",
        "outputId": "2650c77d-1752-41a7-f5f5-9cec8e417981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn<0.24 in /usr/local/lib/python3.7/dist-packages (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.4.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (0.3.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.9.8)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.64.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U 'scikit-learn<0.24'\n",
        "!pip install sklearn-crfsuite\n",
        "\n",
        "# YOU NEED TO RESTART THE RUNTIME!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZteNYk03GPL",
        "outputId": "ef8f23dd-422f-46f4-90a9-cdb646788803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to mount your drive to this notebook in order to read the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du79PuQ7-MD8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os.path import join\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOuZQf_I-PIq"
      },
      "source": [
        "## Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF7EABYQ-JNt"
      },
      "outputs": [],
      "source": [
        "# Put the folder path where the datasets are located\n",
        "PATH = \"/content/drive/My Drive/dataset \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmYh526h-QC1"
      },
      "outputs": [],
      "source": [
        "# function to read data, return list of tuples each tuple represents a token contains word, pos tag, chunk tag, and ner tag\n",
        "import csv\n",
        "def read_data(filename) -> list:\n",
        "  data = []\n",
        "  sentences = []\n",
        "  with open(filename) as load_file:\n",
        "    reader = csv.reader(load_file,delimiter = \" \")   # read\n",
        "   \n",
        "    for row in reader:\n",
        "     \n",
        "      if(len(tuple(row)) != 0):\n",
        "       \n",
        "        if tuple(row) == (' ', 'O', 'O'):\n",
        "          #print(\"STOP\")\n",
        "          new_data = (' \" ',' \" ', 'O', 'O')\n",
        "          data.append(new_data)\n",
        "        elif tuple(row) == (' ', 'O', 'I-PER'):\n",
        "          new_data = (' \" ',' \" ', 'O', 'I-PER')\n",
        "          data.append(new_data)\n",
        "\n",
        "        else:\n",
        "         \n",
        "          data.append(tuple(row))\n",
        "      else:\n",
        "        sentences.append(data)\n",
        "        data  =[]\n",
        "    # add last data because there is no empty line after these data           \n",
        "    if data:\n",
        "       sentences.append(data)\n",
        "    \n",
        "     \n",
        "  \n",
        "    return sentences\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funciton that read data then get Name Entity tag from, this is work as sent2labels\n",
        "\n",
        "def read_data_labels(filename) -> list:\n",
        "  data = []\n",
        "  sentences = []\n",
        "  with open(filename) as load_file:\n",
        "    reader = csv.reader(load_file,delimiter = \" \")   # read\n",
        "   \n",
        "    for row in reader:\n",
        "     \n",
        "      if(len(tuple(row)) != 0):\n",
        "       \n",
        "        if tuple(row) == (' ', 'O', 'O'):\n",
        "          new_data = (' \" ',' \" ', 'O', 'O')\n",
        "          data.append(new_data[3])\n",
        "        elif tuple(row) == (' ', 'O', 'I-PER'):\n",
        "          new_data = (' \" ',' \" ', 'O', 'I-PER')\n",
        "          data.append(new_data[3])\n",
        "          #print(\"STOP\")\n",
        "          \n",
        "        else:\n",
        "         \n",
        "          data.append(tuple(row)[3])\n",
        "      else:\n",
        "        sentences.append(data)\n",
        "        data  =[]\n",
        "    # add last data because there is no empty line after these data           \n",
        "    if data:\n",
        "       sentences.append(data)\n",
        "    \n",
        "     \n",
        "  \n",
        "    return sentences"
      ],
      "metadata": {
        "id": "Q5jbMRwGVgYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgPCBhbj-dSJ"
      },
      "outputs": [],
      "source": [
        "# read data with your custom function\n",
        "train_data = read_data(\"train.txt\")\n",
        "val_data = read_data(\"valid.txt\")\n",
        "test_data = read_data(\"test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    '''\n",
        "      #first_list.append(doc[i][j][3])\n",
        "      #print(\"I m at first list\")\n",
        "      first_list.append(doc[i][j][3])\n",
        "      #print(first_list)\n",
        "    #print(\"I am at second list\")\n",
        "  second_list.append(first_list)     \n",
        "  #second_list.append(first_list) \n",
        "  '''  "
      ],
      "metadata": {
        "id": "u-ofGbzKcXu1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7c4737aa-d1f3-429f-d512-9f21b133aa31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  #first_list.append(doc[i][j][3])\\n  #print(\"I m at first list\")\\n  first_list.append(doc[i][j][3])\\n  #print(first_list)\\n#print(\"I am at second list\")\\n  second_list.append(first_list)     \\n  #second_list.append(first_list) \\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hata bulma fonksiyonu\n",
        "\n",
        "def entitiytager(doc):\n",
        "  \n",
        "  first_list = []\n",
        "  second_list = []\n",
        "  counter = 0\n",
        "  for i in range(len(doc)):\n",
        "    len1 = len(doc[i])\n",
        "    for j in range(len1):\n",
        "      print(doc[i][j][3])\n",
        "      print(i,j)\n",
        "     \n",
        "   \n"
      ],
      "metadata": {
        "id": "GIeTd0sOnQVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[14986][3][3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "66HuFIkJqC1Y",
        "outputId": "5b2b1616-cc5d-4af7-c1e5-cd08cde72bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECYsXDBl-7mx"
      },
      "source": [
        "# Create Gazetteer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK6R3SzM-_6g"
      },
      "outputs": [],
      "source": [
        "# load wikipedia pages\n",
        "\n",
        "import os,json\n",
        "\n",
        "import glob\n",
        "contents = []\n",
        "gazetter= []\n",
        "path_to_json = \"/content/drive/My Drive/wikipedia_pages/\"\n",
        "\n",
        "for file_name in [file for file in os.listdir(path_to_json) if file.endswith('.json')]:\n",
        "  with open(path_to_json + file_name) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    contents.append(data)\n",
        "    given_text =re.findall('&gt;([a-zA-Z]+)&lt;',data['text']) # find interwiki links from the content of the pages\n",
        "    for i in range(len(given_text)):\n",
        "      gazetter.append(given_text[i])\n",
        " \n",
        "    \n",
        "    # you can also define rules to improve your gazetteer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXeoJckq_BK5",
        "outputId": "7745b751-9b11-49b4-a81f-e782176793db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "282331"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# print the size of your gazetteer\n",
        "len(gazetter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hbhw-VC_Bni"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INZH7AlE_Fnw"
      },
      "source": [
        "## Conditional Random Fields (CRF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ac67uI_Ity"
      },
      "source": [
        "### Extract features for CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKh8kvu6_Dze",
        "outputId": "1e25aefb-3fc8-468b-ae1a-b25e10fc2b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "stopwords = stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "utNg2UQ8eZl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions to help create features for CRF\n",
        "\n",
        "\n",
        "def word_shaper(word): #7\n",
        "  shapecreater = \" \"\n",
        "  shortshapecreater = \" \"\n",
        "  for i in range(len(word)):\n",
        "    if word[i].isupper():\n",
        "     shapecreater = shapecreater+'X'\n",
        "    elif word[i].islower():\n",
        "      shapecreater = shapecreater + 'x'\n",
        "    elif word[i].isdigit():\n",
        "      shapecreater = shapecreater+ 'd'\n",
        "    elif word[i] in punctuation:\n",
        "      shapecreater = shapecreater+ word[i]\n",
        "  return shapecreater\n",
        "\n",
        "def short_shaper(word):\n",
        "  shortshapecreater = \" \"\n",
        "  if len(word) == 0:\n",
        "    return shortshapecreater\n",
        "  else:\n",
        "    if len(word) >= 3:\n",
        "      if word[0].isupper() and word[1] == \"’\" and word[2].isupper():\n",
        "        shortshapecreater =  \"X'Xx\"\n",
        "      elif word[0].isupper() == False and word[0] not in punctuation:\n",
        "        shortshapecreater = 'x'\n",
        "      elif word[0].isupper() and word[1] != \"'\":\n",
        "        shortshapecreater = 'Xx' \n",
        "  for i in range(len(word)):\n",
        "     \n",
        "  \n",
        "     if word[i] in punctuation:\n",
        "      shortshapecreater = shortshapecreater+word[i]\n",
        "\n",
        "  return shortshapecreater\n",
        "\n",
        " \n",
        "\n",
        "def number_checker(word):  #9\n",
        "  for i in range(len(word)):\n",
        "    if word[i].isdigit():\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "def hypen_checker(word):  #10\n",
        "  \n",
        "  for i in range(len(word)):\n",
        "    if word[i] == \"-\":\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "\n",
        "def upper_digit_dash(word): #11\n",
        "  upper = False\n",
        "  digit = False\n",
        "  dash = False\n",
        "  if word.isupper():\n",
        "    upper = True\n",
        "  else:\n",
        "    return False\n",
        "  for i in range(len(word)):\n",
        "\n",
        "    if word[i].isdigit():\n",
        "      digit = True\n",
        "    elif word[i] == '-':\n",
        "      dash = True\n",
        "  return (upper and digit and dash)\n",
        "\n",
        "\n",
        "def prefix(word): #12\n",
        "  pre1 = \" \"\n",
        "  pre2 = \" \"\n",
        "  pre3 = \" \"\n",
        "  pre4 = \" \"\n",
        "\n",
        "  for i in range(len(word)):\n",
        "    if i == 0:\n",
        "      pre1 = pre1+word[i]\n",
        "    if i == 1:\n",
        "      pre2 = pre1+word[i]\n",
        "    if i == 2:\n",
        "      pre3 =pre2+word[i]\n",
        "    if i ==3:\n",
        "      pre4 = pre3+word[i]\n",
        " \n",
        "  return pre1,pre2,pre3,pre4\n",
        "\n",
        "def suffix(word): #13\n",
        "  suf1 = \" \"\n",
        "  suf2 = \" \"\n",
        "  suf3 = \" \"\n",
        "  suf4 = \" \"\n",
        "\n",
        "  for i in range(len(word)):\n",
        "    if i ==0:\n",
        "      suf1 = suf1+(word[len(word)-1-i])\n",
        "    if i == 1:\n",
        "      suf2 = suf1+(word[len(word)-1-i])\n",
        "    if i == 2:\n",
        "      suf3 = suf2+(word[len(word)-1-i])\n",
        "    if i == 3:\n",
        "      suf4 = suf3+(word[len(word)-1-i])\n",
        "  return suf1,suf2,suf3,suf4\n",
        "\n",
        "def allupper(word): #14\n",
        "  for i in range(len(word)):\n",
        "    if word[i].isupper() == False:\n",
        "      return False\n",
        "    \n",
        "  return True\n",
        "\n",
        "def stopwordchecker(word): #15\n",
        "  if word in stopwords:\n",
        "    return True\n",
        "  else:\n",
        "   return False\n",
        "\n",
        "def isin_gazetter(word):  #19  #Güncelleme yap\n",
        "\n",
        "  if word in gazetter:\n",
        "   return True         #use dictionary\n",
        "  else:\n",
        "    return False  \n",
        "\n"
      ],
      "metadata": {
        "id": "N0d3MV5MwKAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfHHd0vxq99R"
      },
      "outputs": [],
      "source": [
        "\n",
        "def word2features(doc, i):\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  before_short_shape = \" \"\n",
        "  after_short_shape = \" \"\n",
        " \n",
        " \n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "    before_short_shape = short_shaper(before)\n",
        "   \n",
        "    \n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        "    after_short_shape = short_shaper(after)\n",
        "   \n",
        "    \n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  \n",
        "  \n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  sufixes = suffix(word)\n",
        "  is_all_upper = allupper(word)\n",
        "  is_stopword = stopwordchecker(word)\n",
        "  is_in_gazeeter = isin_gazetter(word)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "   'Contain_Hypen': is_hypen,\n",
        "   'Contain upper, digit and dash': is_up_digit_dash,\n",
        "   'prefix 1':prefixes[0],\n",
        "   'prefix 2':prefixes[1],\n",
        "   'prefix 3':prefixes[2],\n",
        "   'prefix 4':prefixes[3],\n",
        "   'sufix 1': sufixes[0],\n",
        "   'sufix 2': sufixes[1],\n",
        "   'sufix 3': sufixes[2],\n",
        "   'sufix 4': sufixes[3],\n",
        "   'All_upper': is_all_upper,\n",
        "   'Is_stopword':is_stopword,\n",
        "   'Before neighbor words':before,\n",
        "   'After neighbor word':after,\n",
        "   'Before word shape':before_word_shape,\n",
        "   'After word shape':next_word_shape,\n",
        "   'Before short shape':before_short_shape,\n",
        "   'After short shape':after_short_shape,\n",
        "   'Presence of in gazetter':is_in_gazeeter\n",
        "\n",
        "  \n",
        "      \n",
        "  }\n",
        "\n",
        "  return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define function to process each token given a sentence\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        " \n",
        "\n",
        "\n",
        "# get named entity labels from the sentence\n",
        "def sent2labels(doc,i) :                 #Buraya bak\n",
        "  name_tag = doc[i][3]\n",
        "  return name_tag\n",
        "\n",
        "\n",
        "def label_for_sent(sent):\n",
        "  return [sent2labels(sent,i) for i in range(len(sent))]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create functions for each feature to be added"
      ],
      "metadata": {
        "id": "qF-w-GiOh6xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature1(doc,i):\n",
        "  word = doc[i][0]\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word\n",
        "  }\n",
        "\n",
        "  return features\n"
      ],
      "metadata": {
        "id": "4U4iPZFV2GRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature2(doc,i):\n",
        "\n",
        "  word = doc[i][0]\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "  postag = doc[i][1]\n",
        "\n",
        "\n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "  }\n",
        "  return features"
      ],
      "metadata": {
        "id": "rO5Hfun52oyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature3(doc,i):\n",
        "\n",
        "  word = doc[i][0]\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "\n",
        "\n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "  'chunk tag': chunktag\n",
        "  }\n",
        "  return features"
      ],
      "metadata": {
        "id": "ICfQ7WYc2800"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature4(doc,i):\n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  \n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   \n",
        "  }\n",
        "  return features"
      ],
      "metadata": {
        "id": "LGqg4zka3Use"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature5(doc,i):\n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  \n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "  }\n",
        "  return features"
      ],
      "metadata": {
        "id": "iNa1LE0N3yM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature6(doc,i):\n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False \n",
        "   \n",
        "   \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   }\n",
        "  return features"
      ],
      "metadata": {
        "id": "l6CJmGNa4ZyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature7(doc, i):\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        " \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   \n",
        "\n",
        "    #8 add short shape\n",
        "    #17 add short shape of neighbors\n",
        "      \n",
        "  }\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "bWZxJUEr5Q22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature8(doc, i): #Short shape\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        " \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Short_shape':short_shape,\n",
        "   \n",
        "      \n",
        "  }\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "9T4UesMr5al7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature9(doc, i):\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  sufixes = suffix(word)\n",
        "  is_all_upper = allupper(word)\n",
        "  is_stopword = stopwordchecker(word)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "  \n",
        "      \n",
        "  }\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "NFYJM3Jh5n2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature10(doc, i):\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  sufixes = suffix(word)\n",
        "  is_all_upper = allupper(word)\n",
        "  is_stopword = stopwordchecker(word)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "   'Contain_Hypen': is_hypen,\n",
        "\n",
        "      \n",
        "  }\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "k_cSU_Va563f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature11(doc, i):\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  sufixes = suffix(word)\n",
        "  is_all_upper = allupper(word)\n",
        "  is_stopword = stopwordchecker(word)\n",
        " \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "   'Contain_Hypen': is_hypen,\n",
        "   'Contain upper, digit and dash': is_up_digit_dash,\n",
        "  \n",
        "      \n",
        "  }\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "6rQTO7uw6D_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature12(doc, i): #prefix\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "   'Contain_Hypen': is_hypen,\n",
        "   'Contain upper, digit and dash': is_up_digit_dash,\n",
        "   'prefix 1':prefixes[0],\n",
        "   'prefix 2':prefixes[1],\n",
        "   'prefix 3':prefixes[2],\n",
        "   'prefix 4':prefixes[3],\n",
        "   \n",
        "\n",
        " \n",
        "      \n",
        "  }\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "R17MjBKq6MCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature13(doc, i): #sufixes\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  sufixes = suffix(word)\n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "   'Contain_Hypen': is_hypen,\n",
        "   'Contain upper, digit and dash': is_up_digit_dash,\n",
        "   'prefix 1':prefixes[0],\n",
        "   'prefix 2':prefixes[1],\n",
        "   'prefix 3':prefixes[2],\n",
        "   'prefix 4':prefixes[3],\n",
        "   'sufix 1': sufixes[0],\n",
        "   'sufix 2': sufixes[1],\n",
        "   'sufix 3': sufixes[2],\n",
        "   'sufix 4': sufixes[3],\n",
        "   \n",
        "\n",
        "    #8 add short shape\n",
        "    #17 add short shape of neighbors\n",
        "      \n",
        "  }\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "CBqsN_dA6V2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature14(doc, i):\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  sufixes = suffix(word)\n",
        "  is_all_upper = allupper(word)\n",
        "  is_stopword = stopwordchecker(word)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "   'Contain_Hypen': is_hypen,\n",
        "   'Contain upper, digit and dash': is_up_digit_dash,\n",
        "   'prefix 1':prefixes[0],\n",
        "   'prefix 2':prefixes[1],\n",
        "   'prefix 3':prefixes[2],\n",
        "   'prefix 4':prefixes[3],\n",
        "   'sufix 1': sufixes[0],\n",
        "   'sufix 2': sufixes[1],\n",
        "   'sufix 3': sufixes[2],\n",
        "   'sufix 4': sufixes[3],\n",
        "   'All_upper': is_all_upper,\n",
        "   \n",
        "\n",
        "    #8 add short shape\n",
        "    #17 add short shape of neighbors\n",
        "      \n",
        "  }\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "jUzVBFmY6hd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature15(doc, i):\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  sufixes = suffix(word)\n",
        "  is_all_upper = allupper(word)\n",
        "  is_stopword = stopwordchecker(word)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "   'Contain_Hypen': is_hypen,\n",
        "   'Contain upper, digit and dash': is_up_digit_dash,\n",
        "   'prefix 1':prefixes[0],\n",
        "   'prefix 2':prefixes[1],\n",
        "   'prefix 3':prefixes[2],\n",
        "   'prefix 4':prefixes[3],\n",
        "   'sufix 1': sufixes[0],\n",
        "   'sufix 2': sufixes[1],\n",
        "   'sufix 3': sufixes[2],\n",
        "   'sufix 4': sufixes[3],\n",
        "   'All_upper': is_all_upper,\n",
        "   'Is_stopword':is_stopword,\n",
        "  \n",
        "\n",
        "    #8 add short shape\n",
        "    #17 add short shape of neighbors\n",
        "      \n",
        "  }\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "AB43Leql7HCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature16(doc, i):\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  sufixes = suffix(word)\n",
        "  is_all_upper = allupper(word)\n",
        "  is_stopword = stopwordchecker(word)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Hypen': is_hypen,\n",
        "   'Contain upper, digit and dash': is_up_digit_dash,\n",
        "   'prefix 1':prefixes[0],\n",
        "   'prefix 2':prefixes[1],\n",
        "   'prefix 3':prefixes[2],\n",
        "   'prefix 4':prefixes[3],\n",
        "   'sufix 1': sufixes[0],\n",
        "   'sufix 2': sufixes[1],\n",
        "   'sufix 3': sufixes[2],\n",
        "   'sufix 4': sufixes[3],\n",
        "   'All_upper': is_all_upper,\n",
        "   'Is_stopword':is_stopword,\n",
        "   'Before neighbor words':before,\n",
        "   'After neighbor word':after,\n",
        "   \n",
        "\n",
        "    #8 add short shape\n",
        "    #17 add short shape of neighbors\n",
        "      \n",
        "  }\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "Rp4S42Tn6xWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature17(doc, i):\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  #name_entitiy = doc[i][3]\n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  short_shape_before = \"\"\n",
        "  short_shape_after = \"\"\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "    short_shape_before = short_shaper(before)\n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        "    short_shape_after = short_shaper(after)\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  sufixes = suffix(word)\n",
        "  is_all_upper = allupper(word)\n",
        "  is_stopword = stopwordchecker(word)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Hypen': is_hypen,\n",
        "   'Contain upper, digit and dash': is_up_digit_dash,\n",
        "   'prefix 1':prefixes[0],\n",
        "   'prefix 2':prefixes[1],\n",
        "   'prefix 3':prefixes[2],\n",
        "   'prefix 4':prefixes[3],\n",
        "   'sufix 1': sufixes[0],\n",
        "   'sufix 2': sufixes[1],\n",
        "   'sufix 3': sufixes[2],\n",
        "   'sufix 4': sufixes[3],\n",
        "   'All_upper': is_all_upper,\n",
        "   'Is_stopword':is_stopword,\n",
        "   'Before neighbor words':before,\n",
        "   'After neighbor word':after,\n",
        "   'Short shape before':short_shape_before,\n",
        "   \"Short shape after\": short_shape_after,\n",
        "   \n",
        "\n",
        "    #8 add short shape\n",
        "    #17 add short shape of neighbors\n",
        "      \n",
        "  }\n",
        "\n",
        "  \n",
        "  return features"
      ],
      "metadata": {
        "id": "jF2lfrZN8bNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature18(doc, i):\n",
        "  \n",
        "  word = doc[i][0]\n",
        "  postag = doc[i][1]\n",
        "  chunktag = doc[i][2]\n",
        "  \n",
        "  before = \"\"\n",
        "  after = \"\"\n",
        "  short_shape_before = \"\"\n",
        "  short_shape_after = \"\"\n",
        "  short_shape = short_shaper(word)\n",
        "  if i>0:                         #16 Neighbors\n",
        "\n",
        "    beforeword = doc[i-1][0]\n",
        "    before = beforeword\n",
        "    short_shape_before = short_shaper(before)\n",
        "    \n",
        "  \n",
        "  if i != len(doc)-1:\n",
        "    nextword  =doc[i+1][0]\n",
        "    after =nextword\n",
        "    short_shape_after = short_shaper(after)\n",
        "    \n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  stem_word = porter_stemmer.stem(word)\n",
        "\n",
        "  if word == \"-DOCSTART-\":\n",
        "    start_sentence = True\n",
        "  else:\n",
        "    start_sentence = False\n",
        "\n",
        "  if word == '.':\n",
        "    end_sentence = True\n",
        "  else: \n",
        "    end_sentence = False\n",
        "\n",
        "\n",
        "  if word[0].isupper():\n",
        "    is_upper = True\n",
        "  else:\n",
        "    is_upper = False\n",
        "  \n",
        "  word_shape = word_shaper(word)\n",
        "  next_word_shape = word_shaper(after)        #18\n",
        "  before_word_shape = word_shaper(before)     #18\n",
        "  contain_num = number_checker(word)\n",
        "  is_hypen = hypen_checker(word)\n",
        "  is_up_digit_dash = upper_digit_dash(word)\n",
        "  prefixes = prefix(word)\n",
        "  sufixes = suffix(word)\n",
        "  is_all_upper = allupper(word)\n",
        "  is_stopword = stopwordchecker(word)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  features = { \n",
        "  \n",
        "  'stem': stem_word,\n",
        "  'pos_tag': postag,\n",
        "   'chunk tag': chunktag,\n",
        "   'start_of_sentence': start_sentence,\n",
        "   'end_of_sentence': end_sentence,\n",
        "   #'name_entity':name_entitiy,\n",
        "   'Is_uppercase_letter': is_upper,\n",
        "   'Word_Shape': word_shape,\n",
        "   'Short_shape':short_shape,\n",
        "   'Contain_Number': contain_num,\n",
        "   'Contain_Hypen': is_hypen,\n",
        "   'Contain upper, digit and dash': is_up_digit_dash,\n",
        "   'prefix 1':prefixes[0],\n",
        "   'prefix 2':prefixes[1],\n",
        "   'prefix 3':prefixes[2],\n",
        "   'prefix 4':prefixes[3],\n",
        "   'sufix 1': sufixes[0],\n",
        "   'sufix 2': sufixes[1],\n",
        "   'sufix 3': sufixes[2],\n",
        "   'sufix 4': sufixes[3],\n",
        "   'All_upper': is_all_upper,\n",
        "   'Is_stopword':is_stopword,\n",
        "   'Before neighbor words':before,\n",
        "   'After neighbor word':after,\n",
        "   'Before word shape':before_word_shape,\n",
        "   'After word shape':next_word_shape,\n",
        "   'Short shape before':short_shape_before,\n",
        "   'Short shape after': short_shape_after,\n",
        "\n",
        "    #8 add short shape\n",
        "    #17 add short shape of neighbors\n",
        "      \n",
        "  }\n",
        "\n",
        "  return features\n",
        "\n"
      ],
      "metadata": {
        "id": "1LWmHst_6roV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ta1NN3b_vkl"
      },
      "outputs": [],
      "source": [
        "# prepare inputs and labels\n",
        "#train_sents = [sent2features(s) for s in train_data]\n",
        "#val_sents = [sent2features(s) for s in val_data]\n",
        "#test_sents = [sent2features(s) for s in test_data]\n",
        "\n",
        "#train_labels = [sent2labels(s) for s in train_data]\n",
        "#val_labels = [sent2labels(s) for s in val_data]\n",
        "#test_labels = [sent2labels(s) for s in test_data]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare inputs\n",
        "train_sents = [sent2features(s) for s in train_data]\n",
        "val_sents = [sent2features(s) for s in val_data]\n",
        "test_sents = [sent2features(s) for s in test_data]"
      ],
      "metadata": {
        "id": "GG5JHnyU-qTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare labels\n",
        "train_labels = read_data_labels(\"train.txt\")\n",
        "val_labels = read_data_labels(\"valid.txt\")\n",
        "test_labels = read_data_labels(\"test.txt\")\n"
      ],
      "metadata": {
        "id": "Nvh-EAWR2c01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_XPjjvLAWHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85cd07a0-2c7a-4ce1-f78f-a05e05e9fe66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "    keep_tempfiles=None, max_iterations=100)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# Set the hyperparameter space that will be scanned.\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# fitting the model for grid search \n",
        "crf.fit(train_sents, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEE82hj6c_aS",
        "outputId": "c0fe56c3-f047-4cdc-cba4-a521780c36f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(crf.classes_)"
      ],
      "metadata": {
        "id": "C-IWumG7PneU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 , Accuracy and Precision score for the All CRF Features implemented "
      ],
      "metadata": {
        "id": "i43zW0SecddV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlvm45kcAtar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41786d3-c9e7-445b-f1c5-44dae5fc27c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.978895261060282"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# print best parameter after tuning \n",
        "y_pred = crf.predict(val_sents)\n",
        "metrics.flat_f1_score(val_labels, y_pred,\n",
        "                      average='weighted', labels=labels)\n",
        "  \n",
        "\n",
        "#metrics.flat_accuracy_score(val_labels, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy\n",
        "metrics.flat_accuracy_score(val_labels, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3TSNfN2cNyE",
        "outputId": "185d9d55-0833-4841-cc72-46c4c4a72d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9791190042266082"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision\n",
        "metrics.flat_precision_score(val_labels,y_pred,average = \"weighted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEVf7uUicQdX",
        "outputId": "70e0b12c-fc74-435d-adeb-34dd42330186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9788085755191352"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgfYVqnBA1lA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac4f2e2-2d7f-4f33-96af-0324c9db5bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O      0.993     0.997     0.995     42975\n",
            "       B-LOC      0.917     0.929     0.923      1837\n",
            "       I-LOC      0.877     0.860     0.868       257\n",
            "      B-MISC      0.919     0.871     0.894       922\n",
            "      I-MISC      0.871     0.743     0.802       346\n",
            "       B-ORG      0.882     0.834     0.857      1341\n",
            "       I-ORG      0.826     0.830     0.828       751\n",
            "       B-PER      0.915     0.913     0.914      1842\n",
            "       I-PER      0.955     0.966     0.960      1307\n",
            "\n",
            "    accuracy                          0.979     51578\n",
            "   macro avg      0.906     0.882     0.894     51578\n",
            "weighted avg      0.979     0.979     0.979     51578\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# calculate f1-score and classification report for test using sklearn_crfsuite.metrics class\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "print(metrics.flat_classification_report(\n",
        "    val_labels, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ej7DFsdUbJHI",
        "outputId": "9bb31575-635e-4d43-f885-7146c4dbb9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_creater1(train_sentences):\n",
        "  for key in train_sentences:\n",
        "    print(key,train_sentences[key])\n",
        "    "
      ],
      "metadata": {
        "id": "4rgQkxuUOi6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqCdjg67A1on"
      },
      "outputs": [],
      "source": [
        "# start from the stem of the token and add features one by one and train a new model with each feature that you add\n",
        "def sent2features(sent):\n",
        "    return [feature1(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents1 = [sent2features(s) for s in train_data]\n",
        "val_sents1 = [sent2features(s) for s in val_data]\n",
        "test_sents1 = [sent2features(s) for s in test_data]\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature2(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents2 = [sent2features(s) for s in train_data]\n",
        "val_sents2 = [sent2features(s) for s in val_data]\n",
        "test_sents2 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature3(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents3 = [sent2features(s) for s in train_data]\n",
        "val_sents3 = [sent2features(s) for s in val_data]\n",
        "test_sents3 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature4(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents4 = [sent2features(s) for s in train_data]\n",
        "val_sents4 = [sent2features(s) for s in val_data]\n",
        "test_sents4 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature5(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents5 = [sent2features(s) for s in train_data]\n",
        "val_sents5 = [sent2features(s) for s in val_data]\n",
        "test_sents5 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature6(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents6 = [sent2features(s) for s in train_data]\n",
        "val_sents6 = [sent2features(s) for s in val_data]\n",
        "test_sents6 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature7(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents7 = [sent2features(s) for s in train_data]\n",
        "val_sents7 = [sent2features(s) for s in val_data]\n",
        "test_sents7 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature8(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents8 = [sent2features(s) for s in train_data]\n",
        "val_sents8 = [sent2features(s) for s in val_data]\n",
        "test_sents8 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature9(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents9 = [sent2features(s) for s in train_data]\n",
        "val_sents9 = [sent2features(s) for s in val_data]\n",
        "test_sents9 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature10(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents10 = [sent2features(s) for s in train_data]\n",
        "val_sents10 = [sent2features(s) for s in val_data]\n",
        "test_sents10 = [sent2features(s) for s in test_data]\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature11(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents11 = [sent2features(s) for s in train_data]\n",
        "val_sents11 = [sent2features(s) for s in val_data]\n",
        "test_sents11 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature12(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents12 = [sent2features(s) for s in train_data]\n",
        "val_sents12 = [sent2features(s) for s in val_data]\n",
        "test_sents12 = [sent2features(s) for s in test_data]\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature13(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents13 = [sent2features(s) for s in train_data]\n",
        "val_sents13 = [sent2features(s) for s in val_data]\n",
        "test_sents13 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature14(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents14 = [sent2features(s) for s in train_data]\n",
        "val_sents14 = [sent2features(s) for s in val_data]\n",
        "test_sents14 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature15(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents15 = [sent2features(s) for s in train_data]\n",
        "val_sents15 = [sent2features(s) for s in val_data]\n",
        "test_sents15 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature16(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents16 = [sent2features(s) for s in train_data]\n",
        "val_sents16 = [sent2features(s) for s in val_data]\n",
        "test_sents16 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature17(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents17 = [sent2features(s) for s in train_data]\n",
        "val_sents17 = [sent2features(s) for s in val_data]\n",
        "test_sents17 = [sent2features(s) for s in test_data]\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [feature18(sent, i) for i in range(len(sent))]\n",
        "\n",
        "train_sents18 = [sent2features(s) for s in train_data]\n",
        "val_sents18 = [sent2features(s) for s in val_data]\n",
        "test_sents18 = [sent2features(s) for s in test_data]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# fitting the model for grid search \n",
        "crf.fit(train_sents1, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNBJcSizd2Gy",
        "outputId": "1d64d86d-c478-45ad-c46f-55ea96ac01b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "    keep_tempfiles=None, max_iterations=100)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_sent_list = [train_sents1,train_sents2,train_sents3,train_sents4,train_sents5,train_sents6,train_sents7,train_sents8,train_sents9,train_sents10,train_sents11,train_sents12,train_sents13,train_sents14,train_sents15,train_sents16,train_sents17,train_sents18]"
      ],
      "metadata": {
        "id": "_IBkrhDRge3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_sent_list = [val_sents1,val_sents2,val_sents3,val_sents4,val_sents5,val_sents6,val_sents7,val_sents8,val_sents9,val_sents10,val_sents11,val_sents12,val_sents13,val_sents14,val_sents15,val_sents16,val_sents17,val_sents18]"
      ],
      "metadata": {
        "id": "FxjhVnnloA0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crf.fit(train_sents4,train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-s3t9rMiSWz",
        "outputId": "7dfdb065-b3a4-47b0-c269-543502afeeca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "    keep_tempfiles=None, max_iterations=100)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHyj6PzMBzV6"
      },
      "outputs": [],
      "source": [
        "# display the result table\n",
        "dfFeatures = {'features':['Stem','+Pos','+Chunk','+BOS','+EOS','+isUpper','+shape','+shortShape','+number','+hypen','+upper_digit and dash','+prefixes','+suffixes','+allUpper','+Stopword','+Neighbords','+sShape Neighbor','+wShape Neighbor','+inGazetter'],\n",
        "              'F1_Score':[],\n",
        "              'Accuracy':[],\n",
        "              'Precision': []\n",
        "              \n",
        "              }\n",
        "for i in range(len(training_sent_list)):\n",
        "  crf.fit(training_sent_list[i],train_labels)\n",
        "\n",
        "\n",
        "  y_pred = crf.predict(valid_sent_list[i])\n",
        "  f1_score=metrics.flat_f1_score(val_labels, y_pred,\n",
        "                      average='weighted', labels=labels)\n",
        "\n",
        "  accuracy =metrics.flat_accuracy_score(val_labels, y_pred)\n",
        "  precision =metrics.flat_precision_score(val_labels,y_pred,average = \"weighted\")\n",
        "\n",
        "  \n",
        "  dfFeatures['F1_Score'].append(f1_score)\n",
        "  dfFeatures['Accuracy'].append(accuracy)\n",
        "  dfFeatures['Precision'].append(precision)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crf.fit(train_sents,train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGv2mG1OnvQB",
        "outputId": "a3042008-6ac6-4683-af76-0f8aac9d4264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "    keep_tempfiles=None, max_iterations=100)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = crf.predict(val_sents)\n",
        "f1_score=metrics.flat_f1_score(val_labels, y_pred,\n",
        "                      average='weighted', labels=labels)\n",
        "accuracy =metrics.flat_accuracy_score(val_labels, y_pred)\n",
        "precision =metrics.flat_precision_score(val_labels,y_pred,average = \"weighted\")\n",
        "\n",
        "print(f1_score,' ',accuracy,' ',precision)\n",
        "dfFeatures['F1_Score'].append(f1_score)\n",
        "dfFeatures['Accuracy'].append(accuracy)\n",
        "dfFeatures['Precision'].append(precision)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hup7znJWn1hv",
        "outputId": "b976af4a-70bd-4adc-dabd-4e35d5111eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.978895261060282   0.9791190042266082   0.9788085755191352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfFeatures = pd.DataFrame(dfFeatures)\n",
        "dfFeatures"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "xz9bzlSNX4no",
        "outputId": "3b187011-9ca1-4760-c8e4-e421ea7ae7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 features  F1_Score  Accuracy  Precision\n",
              "0                    Stem  0.935849  0.939083   0.936609\n",
              "1                    +Pos  0.961719  0.962465   0.962056\n",
              "2                  +Chunk  0.962823  0.963337   0.963148\n",
              "3                    +BOS  0.962680  0.963182   0.963015\n",
              "4                    +EOS  0.962763  0.963240   0.963128\n",
              "5                +isUpper  0.966670  0.966497   0.967505\n",
              "6                  +shape  0.970604  0.970802   0.970587\n",
              "7             +shortShape  0.970775  0.970976   0.970807\n",
              "8                 +number  0.970910  0.971054   0.970943\n",
              "9                  +hypen  0.971161  0.971344   0.971181\n",
              "10  +upper_digit and dash  0.970723  0.970918   0.970714\n",
              "11              +prefixes  0.971090  0.971209   0.971069\n",
              "12              +suffixes  0.973086  0.973244   0.973006\n",
              "13              +allUpper  0.973040  0.973186   0.972977\n",
              "14              +Stopword  0.972693  0.972818   0.972659\n",
              "15            +Neighbords  0.976711  0.976967   0.976607\n",
              "16       +sShape Neighbor  0.978697  0.978945   0.978630\n",
              "17       +wShape Neighbor  0.978841  0.979041   0.978784\n",
              "18            +inGazetter  0.978895  0.979119   0.978809"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa68b6a4-4205-4f21-be5d-47be63584b14\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>F1_Score</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stem</td>\n",
              "      <td>0.935849</td>\n",
              "      <td>0.939083</td>\n",
              "      <td>0.936609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>+Pos</td>\n",
              "      <td>0.961719</td>\n",
              "      <td>0.962465</td>\n",
              "      <td>0.962056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>+Chunk</td>\n",
              "      <td>0.962823</td>\n",
              "      <td>0.963337</td>\n",
              "      <td>0.963148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>+BOS</td>\n",
              "      <td>0.962680</td>\n",
              "      <td>0.963182</td>\n",
              "      <td>0.963015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>+EOS</td>\n",
              "      <td>0.962763</td>\n",
              "      <td>0.963240</td>\n",
              "      <td>0.963128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>+isUpper</td>\n",
              "      <td>0.966670</td>\n",
              "      <td>0.966497</td>\n",
              "      <td>0.967505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>+shape</td>\n",
              "      <td>0.970604</td>\n",
              "      <td>0.970802</td>\n",
              "      <td>0.970587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>+shortShape</td>\n",
              "      <td>0.970775</td>\n",
              "      <td>0.970976</td>\n",
              "      <td>0.970807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>+number</td>\n",
              "      <td>0.970910</td>\n",
              "      <td>0.971054</td>\n",
              "      <td>0.970943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>+hypen</td>\n",
              "      <td>0.971161</td>\n",
              "      <td>0.971344</td>\n",
              "      <td>0.971181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>+upper_digit and dash</td>\n",
              "      <td>0.970723</td>\n",
              "      <td>0.970918</td>\n",
              "      <td>0.970714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>+prefixes</td>\n",
              "      <td>0.971090</td>\n",
              "      <td>0.971209</td>\n",
              "      <td>0.971069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>+suffixes</td>\n",
              "      <td>0.973086</td>\n",
              "      <td>0.973244</td>\n",
              "      <td>0.973006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>+allUpper</td>\n",
              "      <td>0.973040</td>\n",
              "      <td>0.973186</td>\n",
              "      <td>0.972977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>+Stopword</td>\n",
              "      <td>0.972693</td>\n",
              "      <td>0.972818</td>\n",
              "      <td>0.972659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>+Neighbords</td>\n",
              "      <td>0.976711</td>\n",
              "      <td>0.976967</td>\n",
              "      <td>0.976607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>+sShape Neighbor</td>\n",
              "      <td>0.978697</td>\n",
              "      <td>0.978945</td>\n",
              "      <td>0.978630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>+wShape Neighbor</td>\n",
              "      <td>0.978841</td>\n",
              "      <td>0.979041</td>\n",
              "      <td>0.978784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>+inGazetter</td>\n",
              "      <td>0.978895</td>\n",
              "      <td>0.979119</td>\n",
              "      <td>0.978809</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa68b6a4-4205-4f21-be5d-47be63584b14')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aa68b6a4-4205-4f21-be5d-47be63584b14 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aa68b6a4-4205-4f21-be5d-47be63584b14');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that from the table best model is 18 which is +inGazetter.\n",
        "And now we will check classficiation report with test set."
      ],
      "metadata": {
        "id": "A-o85zOk-SBm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvZ-Qr14BzZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e43b1a1-8ddd-462d-9ca2-4ef18c288929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O      0.991     0.988     0.989     38554\n",
            "       B-LOC      0.858     0.885     0.871      1668\n",
            "       I-LOC      0.773     0.755     0.764       257\n",
            "      B-MISC      0.808     0.789     0.798       702\n",
            "      I-MISC      0.576     0.685     0.626       216\n",
            "       B-ORG      0.822     0.760     0.790      1661\n",
            "       I-ORG      0.707     0.758     0.732       835\n",
            "       B-PER      0.855     0.863     0.859      1617\n",
            "       I-PER      0.880     0.962     0.919      1156\n",
            "\n",
            "    accuracy                          0.961     46666\n",
            "   macro avg      0.808     0.827     0.817     46666\n",
            "weighted avg      0.962     0.961     0.961     46666\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# display the classification report for the best model\n",
        "crf.fit(train_sents,train_labels)\n",
        "\n",
        "y_pred = crf.predict(test_sents)\n",
        "\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "print(metrics.flat_classification_report(\n",
        "    test_labels, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4DNBgU6B4Qs"
      },
      "source": [
        "## Recurrent Neural Network (RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6b7iz44B6kb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Model, Input, Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, Input, Dropout, LSTM, TimeDistributed, Bidirectional\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfVSNob9Ub_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a326e19-e986-4482-8df8-69a5049c3c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "Q4Enmt5MUxqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_words = [[s[0] for s in w] for w in train_data ]\n",
        "val_words = [[s[0] for s in w] for w in val_data ]\n",
        "test_words = [[s[0] for s in w] for w in test_data ]\n",
        "labels_ = [[s[3] for s in w] for w in train_data ]\n",
        "labels_for_test =[[s[3] for s in w] for w in test_data ] "
      ],
      "metadata": {
        "id": "cbSxoNkyAPgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_labels.append('Others')\n",
        "numlist = [0,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "labels_dict = dict(zip(numlist,sorted_labels))"
      ],
      "metadata": {
        "id": "V6yhwKBdF1Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJS23L_bJjwG",
        "outputId": "25fb5869-f0ba-47de-847d-75cdfa5f6282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-LOC',\n",
              " 2: 'I-LOC',\n",
              " 3: 'B-MISC',\n",
              " 4: 'I-MISC',\n",
              " 5: 'B-ORG',\n",
              " 6: 'I-ORG',\n",
              " 7: 'B-PER',\n",
              " 8: 'I-PER',\n",
              " 9: 'Others'}"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVMM7jLV8ch7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64a746e-d095-4d60-8a0f-83520732ffc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Create your own word embeddings from scratch and load a pretrained word embeddings\n",
        "corpus = api.load('text8')\n",
        "                                            \n",
        "\n",
        "pre_embedding = Word2Vec(corpus,window = 5,min_count = 1,workers = 4)\n",
        "# You can check https://radimrehurek.com/gensim/models/word2vec.html for training a word embeddings from scratch\n",
        "\n",
        "# You can check https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html and https://github.com/RaRe-Technologies/gensim-data for loading pretrained word embeddings. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_s = Word2Vec(sentences=train_words, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "DNObrIB_Ayhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer_label = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(train_words)\n",
        "tokenizer_label.fit_on_texts(labels_)\n",
        "\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index # Tuple that contains words and their corresponding number\n",
        "word_index_label  =tokenizer_label.word_index"
      ],
      "metadata": {
        "id": "zBb6z-SgBHTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GujDr8TjOv2t",
        "outputId": "249095c9-a174-49b3-f906-7d202933229e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b-loc': 2,\n",
              " 'b-misc': 7,\n",
              " 'b-org': 4,\n",
              " 'b-per': 3,\n",
              " 'i-loc': 8,\n",
              " 'i-misc': 9,\n",
              " 'i-org': 6,\n",
              " 'i-per': 5,\n",
              " 'o': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11c6tUhBbLAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_words = len(list(tokenizer.word_index))\n",
        "print(\"Number of word from our training word is:\",num_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vLIMwacBrjG",
        "outputId": "89881e1e-7baf-46a4-970e-88e15e6d7782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of word from our training word is: 21010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequence = tokenizer.texts_to_sequences(train_words)\n",
        "val_sequence = tokenizer.texts_to_sequences(val_words)\n",
        "test_sequence = tokenizer.texts_to_sequences(test_words)\n",
        "\n",
        "\n",
        "label_sequence = tokenizer_label.texts_to_sequences(labels_)\n",
        "# Return the list of sentences that contains their cooresding values, word--> sentences which labelled in word_index\n",
        "tokenizer_label.fit_on_texts(labels_for_test)\n",
        "test_label_sequence = tokenizer_label.texts_to_sequences(labels_for_test)"
      ],
      "metadata": {
        "id": "MWQcdUo3BzOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYHhtyMgKBsG",
        "outputId": "66c4592f-f03c-44f6-e99d-5f1726b70028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER']"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify max length of token in our sequences \n",
        "max_length = 0\n",
        "for index in range(len(train_sequence)):\n",
        "  numberofwords=len(train_sequence[index])\n",
        "  if (numberofwords) > (max_length):\n",
        "    max_length = numberofwords\n",
        "print(max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IvihRxCEetY",
        "outputId": "82086478-bd9a-4090-c7c7-1348ac8df3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPMw7vkw8ckn"
      },
      "outputs": [],
      "source": [
        "# preprare your dataset for RNN classifier (you need to add padding to labels as well)\n",
        "\n",
        "train_pad = pad_sequences(train_sequence, maxlen=max_length,padding='post')\n",
        "valid_pad = pad_sequences(val_sequence, maxlen=max_length,padding='post') \n",
        "test_pad = pad_sequences(test_sequence, maxlen=max_length,padding='post')  \n",
        "\n",
        "label_pad = pad_sequences(label_sequence, maxlen=max_length,padding='post')\n",
        "\n",
        "test_label_pad = pad_sequences(test_label_sequence,maxlen=max_length,padding='post')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding matrix  from our train data"
      ],
      "metadata": {
        "id": "u-dsuvREYGSp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1rZ6jb18cpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebcf8c41-2729-41db-cf83-88d45e57bc87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings Matrix shape of trained data:  (21011, 100)\n"
          ]
        }
      ],
      "source": [
        "# Create Embedding Matrices and Layers\n",
        "\n",
        "unique_words = len(word_index)\n",
        "total_words = unique_words + 1\n",
        "skipped_words = 0\n",
        "embedding_dim = 100  \n",
        "embedding_matrix = np.zeros((total_words, embedding_dim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  try:\n",
        "    embedding_vector = embedding_s[word]\n",
        "  except:\n",
        "    skipped_words = skipped_words+1\n",
        "    pass\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[index] = embedding_vector\n",
        "print(\"Embeddings Matrix shape of trained data: \",embedding_matrix.shape)\n",
        "\n",
        "\n",
        "embedding_layer = Embedding(total_words, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_tags = len(sorted_labels)"
      ],
      "metadata": {
        "id": "8WFyFoiHRnVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models trained from embedding layer as training set"
      ],
      "metadata": {
        "id": "ChkKMpsyZtuw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DbhQz268cs8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c225a47b-60a6-4977-d3c5-8c348f938f54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 113, 100)          2101100   \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirecti  (None, 113, 200)         160800    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 113, 100)          120400    \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDis  (None, 113, 10)          1010      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,383,310\n",
            "Trainable params: 282,210\n",
            "Non-trainable params: 2,101,100\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Create your models and train them\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "\n",
        "model.add(Bidirectional(LSTM(units=embedding_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
        "\n",
        "model.add(LSTM(units=embedding_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
        "\n",
        "model.add(TimeDistributed(Dense(n_tags+1, activation=\"relu\")))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_pad, label_pad,epochs=5,batch_size=1000,verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0eryCFwU3iq",
        "outputId": "c0bc4342-585b-4d23-cd4c-6e32fa3e466e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "15/15 [==============================] - 153s 10s/step - loss: 0.0827 - accuracy: 0.9794\n",
            "Epoch 2/5\n",
            "15/15 [==============================] - 150s 10s/step - loss: 0.0821 - accuracy: 0.9795\n",
            "Epoch 3/5\n",
            "15/15 [==============================] - 148s 10s/step - loss: 0.0814 - accuracy: 0.9796\n",
            "Epoch 4/5\n",
            "15/15 [==============================] - 149s 10s/step - loss: 0.0808 - accuracy: 0.9798\n",
            "Epoch 5/5\n",
            "15/15 [==============================] - 149s 10s/step - loss: 0.0803 - accuracy: 0.9799\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa36698e10>"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_pad, test_label_pad, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GI3j8oPaWLO",
        "outputId": "c52c10a5-4cdb-4aa8-da10-299391dd7755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_label_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0iiFzYzwcL-",
        "outputId": "0c7d2919-0c27-49be-b1ea-58dacc412190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 0],\n",
              "       [4, 1, 7, ..., 0, 0, 0],\n",
              "       [3, 5, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [4, 1, 4, ..., 0, 0, 0],\n",
              "       [1, 1, 0, ..., 0, 0, 0],\n",
              "       [4, 1, 4, ..., 0, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(test_pad)\n",
        "\n",
        "labels = np.argmax(y_pred, axis=1) "
      ],
      "metadata": {
        "id": "w3kDn7F8eMA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv-rwcFKU0G3"
      },
      "outputs": [],
      "source": [
        "# define a function to remove paddings and align labels and tokens\n",
        "def align_predictions(predictions:np.array, label_ids:np.array) -> list, list:\n",
        "  pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzEkCwJ58cvh"
      },
      "outputs": [],
      "source": [
        "# Evaluate your models with functions of seqeval library\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyU-zQLx8cx9"
      },
      "outputs": [],
      "source": [
        "# ...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDnYCXXEUG_g"
      },
      "source": [
        "## My Report\n",
        "We have two different models which first one if Conditional Random Fields(CRF) and Reccurent Neural Network(RNN), we use this models to NER.\n",
        "\n",
        "# **CRF Model:**\n",
        "\n",
        "In this model first we create a gazetter from the 20,000 wikipedia pages. Then use this gazeeter to add feature in crf model. We trained 19 different models for crf which we add one by one new feature to each model  in order to see how well our model is learning for each feature.\n",
        "At the end of the all traning models we set that the last one which is checks all the features that lastly add \"+inGazetter\" feauture and evaluate in the test data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfFeatures"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "lUnJ4IBbll4m",
        "outputId": "7176659e-e52b-438d-e694-754419671464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 features  F1_Score  Accuracy  Precision\n",
              "0                    Stem  0.935849  0.939083   0.936609\n",
              "1                    +Pos  0.961719  0.962465   0.962056\n",
              "2                  +Chunk  0.962823  0.963337   0.963148\n",
              "3                    +BOS  0.962680  0.963182   0.963015\n",
              "4                    +EOS  0.962763  0.963240   0.963128\n",
              "5                +isUpper  0.966670  0.966497   0.967505\n",
              "6                  +shape  0.970604  0.970802   0.970587\n",
              "7             +shortShape  0.970775  0.970976   0.970807\n",
              "8                 +number  0.970910  0.971054   0.970943\n",
              "9                  +hypen  0.971161  0.971344   0.971181\n",
              "10  +upper_digit and dash  0.970723  0.970918   0.970714\n",
              "11              +prefixes  0.971090  0.971209   0.971069\n",
              "12              +suffixes  0.973086  0.973244   0.973006\n",
              "13              +allUpper  0.973040  0.973186   0.972977\n",
              "14              +Stopword  0.972693  0.972818   0.972659\n",
              "15            +Neighbords  0.976711  0.976967   0.976607\n",
              "16       +sShape Neighbor  0.978697  0.978945   0.978630\n",
              "17       +wShape Neighbor  0.978841  0.979041   0.978784\n",
              "18            +inGazetter  0.978895  0.979119   0.978809"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-babe4396-19ce-403e-8ae9-03e83a8d8d29\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>F1_Score</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stem</td>\n",
              "      <td>0.935849</td>\n",
              "      <td>0.939083</td>\n",
              "      <td>0.936609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>+Pos</td>\n",
              "      <td>0.961719</td>\n",
              "      <td>0.962465</td>\n",
              "      <td>0.962056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>+Chunk</td>\n",
              "      <td>0.962823</td>\n",
              "      <td>0.963337</td>\n",
              "      <td>0.963148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>+BOS</td>\n",
              "      <td>0.962680</td>\n",
              "      <td>0.963182</td>\n",
              "      <td>0.963015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>+EOS</td>\n",
              "      <td>0.962763</td>\n",
              "      <td>0.963240</td>\n",
              "      <td>0.963128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>+isUpper</td>\n",
              "      <td>0.966670</td>\n",
              "      <td>0.966497</td>\n",
              "      <td>0.967505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>+shape</td>\n",
              "      <td>0.970604</td>\n",
              "      <td>0.970802</td>\n",
              "      <td>0.970587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>+shortShape</td>\n",
              "      <td>0.970775</td>\n",
              "      <td>0.970976</td>\n",
              "      <td>0.970807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>+number</td>\n",
              "      <td>0.970910</td>\n",
              "      <td>0.971054</td>\n",
              "      <td>0.970943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>+hypen</td>\n",
              "      <td>0.971161</td>\n",
              "      <td>0.971344</td>\n",
              "      <td>0.971181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>+upper_digit and dash</td>\n",
              "      <td>0.970723</td>\n",
              "      <td>0.970918</td>\n",
              "      <td>0.970714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>+prefixes</td>\n",
              "      <td>0.971090</td>\n",
              "      <td>0.971209</td>\n",
              "      <td>0.971069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>+suffixes</td>\n",
              "      <td>0.973086</td>\n",
              "      <td>0.973244</td>\n",
              "      <td>0.973006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>+allUpper</td>\n",
              "      <td>0.973040</td>\n",
              "      <td>0.973186</td>\n",
              "      <td>0.972977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>+Stopword</td>\n",
              "      <td>0.972693</td>\n",
              "      <td>0.972818</td>\n",
              "      <td>0.972659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>+Neighbords</td>\n",
              "      <td>0.976711</td>\n",
              "      <td>0.976967</td>\n",
              "      <td>0.976607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>+sShape Neighbor</td>\n",
              "      <td>0.978697</td>\n",
              "      <td>0.978945</td>\n",
              "      <td>0.978630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>+wShape Neighbor</td>\n",
              "      <td>0.978841</td>\n",
              "      <td>0.979041</td>\n",
              "      <td>0.978784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>+inGazetter</td>\n",
              "      <td>0.978895</td>\n",
              "      <td>0.979119</td>\n",
              "      <td>0.978809</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-babe4396-19ce-403e-8ae9-03e83a8d8d29')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-babe4396-19ce-403e-8ae9-03e83a8d8d29 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-babe4396-19ce-403e-8ae9-03e83a8d8d29');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we check the F1, Accuracy and Precision score for all models, firstly we began with only stemming of word which gives scores as:\n",
        "\n",
        "               F1         Accuracy  Precision\n",
        "\n",
        "      Stem\t0.935849\t0.939083\t0.936609\n",
        "\n",
        "  \n",
        "This scores are good values to start, and we can see that mostly our scores are goes up for new features or stays similar to last ones.\n",
        "\n",
        "For instance: \n",
        "\n",
        "  \n",
        "    features\tF1_Score\tAccuracy\tPrecision\n",
        "    0\tStem\t0.935849\t0.939083\t0.936609\n",
        "    1\t+Pos\t0.961719\t0.962465\t0.962056\n",
        "    2\t+Chunk\t0.962823\t0.963337\t0.963148\n",
        "    3\t+BOS\t0.962680\t0.963182\t0.963015\n",
        "\n",
        "\n",
        "At the end of the +BOS our f1, accuracy and precision scores are decreased a little to 0.0002 which is not affect the model hugely.\n",
        "\n",
        "We have two main points that features affect positively to our model which first one is:\n",
        "\n",
        "        features\tF1_Score\tAccuracy\tPrecision\n",
        "    5\t+isUpper\t0.966670\t0.966497\t0.967505\n",
        "    6\t+shape\t0.970604\t0.970802\t0.970587\n",
        "\n",
        "\n",
        "we can see that after we add +sShape our model score are changes by 0.004 which normally we can not see that much increase.\n",
        "\n",
        "and secondly\n",
        "\n",
        "\n",
        "        features\tF1_Score\tAccuracy\tPrecision\n",
        "    14\t+Stopword\t0.972693\t0.972818\t0.972659\n",
        "    15\t+Neighbords\t0.976711\t0.976967\t0.976607\n",
        "\n",
        "again there is score increase by 0.0041 by adding neighbord words of that given words.\n",
        "\n",
        "At the end we can see our last model is the best one and from the beggining with the **0.935** F1 score is up to **0.978** with the help of the 19 different features.\n",
        "\n",
        "  After this step we trained best model from above with the test data and report as:\n",
        "\n",
        "                  precision    recall  f1-score   support\n",
        "\n",
        "           O      0.991     0.988     0.989     38554\n",
        "       B-LOC      0.858     0.885     0.871      1668\n",
        "       I-LOC      0.773     0.755     0.764       257\n",
        "      B-MISC      0.808     0.789     0.798       702\n",
        "      I-MISC      0.576     0.685     0.626       216\n",
        "       B-ORG      0.822     0.760     0.790      1661\n",
        "       I-ORG      0.707     0.758     0.732       835\n",
        "       B-PER      0.855     0.863     0.859      1617\n",
        "       I-PER      0.880     0.962     0.919      1156\n",
        "\n",
        "    accuracy                          0.961     46666\n",
        "    macro avg      0.808     0.827     0.817     46666\n",
        "    weighted avg      0.962     0.961     0.961     46666\n",
        "\n",
        "\n",
        "We can now check each scores for each tags in test data. We have 4 scores as 0,I-PER,B-LOCK and B-PER. Since we have lots of 0 which we can also see from the support. We expect to label 0 with this close accuracy.\n",
        "\n",
        "We have worst scores as I-MISC, I-ORG,I-LOC which f1 scores as 0.62,0.73, 0.76.\n",
        "The interesting part is even if the I-MISC AND I-LOC has similar support values, their their precision and f1 values are differ from each other.\n",
        "\n",
        "Our model is not good as label I-MISC but we can not say that this is because of our data did not contains I-MISC tags since our  I-LOC values are better than I-MISC.\n",
        "\n",
        "RNN MODEL:\n",
        "\n",
        "For rnn, we again do similar things as CNN, word embeding and layers are changes. I can only do one rnn model to fit with the word embedding strategy as from our training data. Rnn is differ from cnn with the Bidirectional and Time Distributed layer\n",
        "and accuracy of this model is\n",
        "\n",
        " Accuracy: 97.03%"
      ],
      "metadata": {
        "id": "OZKw0dV0lr-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Refferences\n",
        "\n",
        "https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html \n",
        "\n",
        "\n",
        "https://colab.research.google.com/drive/1FO48RJgao6_w8cICPKNsveisKz-OFsPP#scrollTo=oCT-Zt3ZMocG (From the first project )"
      ],
      "metadata": {
        "id": "lx4Cnpz1sc5x"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}